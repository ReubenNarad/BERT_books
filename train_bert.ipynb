{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7732b8-6dbc-484a-860d-eadeb7a59eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine Tuning BERT\n",
    "adapted from: https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=4wxY3x-ZZz8h\n",
    "\n",
    "## Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cda596f-580d-4993-a760-04d8a652fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a1ff5-a3f2-4ea2-b33c-0ac6c709b5c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d32371-a373-4b38-8fb4-a963187c9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files={'train': './data/book/train.json', 'validation': './data/book/valid.json'})\n",
    "\n",
    "# Subset the first 300 rows of the training data\n",
    "dataset['train'] = dataset['train'].select(range(250))\n",
    "\n",
    "# Subset the first 150 rows of the validation data\n",
    "dataset['validation'] = dataset['validation'].select(range(250))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd1bf1-f19c-4c7e-bbc2-e4f475191f10",
   "metadata": {},
   "source": [
    "## Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19b9b1-5243-47ae-a8b4-6863f19c1977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18faee47-1db5-45fb-aeda-eb84e47c23e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'User Preference: \"Q Is for Quarry\" written by Sue Grafton, \"The End of Enemies (Briggs Tanner Novels)\" written by Grant Blackwood\\nUser Unpreference: \"ICEFIRE\" written by Judith Reeves-Stevens\\nWhether the user will like the target book \"\"Specter of the Past: Star Wars (Star Wars (Bantam Books (Firm) : Unnumbered).)\" written by Timothy Zahn\"?',\n",
       " 'instruction': 'Given the user\\'s preference and unpreference, identify whether the user will like the target book by answering \"Yes.\" or \"No.\".',\n",
       " 'output': 'Yes.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcc4a0-9e44-4540-b186-287c4e70af05",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7435b1-badc-4f48-a644-473832898cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['input', 'instruction']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18c40b-7166-4ac9-8573-eee034c5fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = [x + y for x, y in zip(examples[\"instruction\"], examples['input'])]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  \n",
    "    # convert 'Yes' or 'No' to binary labels\n",
    "    labels_batch = examples['output']\n",
    "    # Binary encode the labels ('No' -> 0 and 'Yes' -> 1)\n",
    "    labels_encoded = torch.tensor([1 if label == 'Yes.' else 0 for label in labels_batch], dtype=torch.float)\n",
    "\n",
    "    encoding[\"labels\"] = labels_encoded\n",
    "  \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd05b10-40b3-49da-b2a8-ca588096f31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eb6a28cd3b46dc9c059356b6525ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f48935-8b5a-4499-91a3-a1a38692f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c3eac-1972-4d75-811e-ff83e9ef76eb",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4137c26-01b6-41aa-959d-43d0beb6302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f4e73-d95c-447d-bcf8-5ae2a006ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0}\n",
      "{0.0, 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Unwrap tensors into a list of integers for both training and validation sets\n",
    "train_labels = [label.item() for label in encoded_dataset['train']['labels']]\n",
    "validation_labels = [label.item() for label in encoded_dataset['validation']['labels']]\n",
    "\n",
    "print(set(train_labels))\n",
    "print(set(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd6484-ab18-4b40-86a1-8893486c5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0850b-3399-43a0-aa12-99a01bd98aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a2a4f-f463-4be5-9600-4001cbae4dce",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a091fbc6-5f1a-48e5-b212-7a97947e8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def binary_metrics(predictions, labels):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # Use threshold to turn probabilities into binary predictions\n",
    "    y_pred = (probs >= 0.5).long()\n",
    "    # Compute metrics\n",
    "    f1 = f1_score(y_true=labels, y_pred=y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true=labels, y_score=probs)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=y_pred)\n",
    "    metrics = {'f1': f1, 'roc_auc': roc_auc, 'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    # Convert the logits to class probabilities using the sigmoid function\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(preds)).numpy()\n",
    "    \n",
    "    # Flatten the predictions and labels since we're dealing with binary classification\n",
    "    flatten_labels = p.label_ids.flatten()\n",
    "    flatten_preds = probs.flatten()\n",
    "    \n",
    "    # Now use the binary_metrics function to calculate the binary classification metrics\n",
    "    result = binary_metrics(predictions=flatten_preds, labels=flatten_labels)\n",
    "    \n",
    "    # You can log these metrics or return them\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5575970-b600-485f-bd72-a12326859404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.3963, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[0.3705]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "#forward pass\n",
    "\n",
    "# Get the input ids from the first example of the training dataset\n",
    "input_ids = encoded_dataset['train']['input_ids'][0].unsqueeze(0)\n",
    "labels = torch.tensor([encoded_dataset['train']['labels'][0]]).unsqueeze(0)\n",
    "labels = labels.float()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move the model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Move the input tensors to the same device as the model\n",
    "input_ids = input_ids.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "\n",
    "# Perform the forward pass\n",
    "outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294d4cd-5a12-4b7a-8b01-5e9e1a1c7ab1",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33e6b291-9aed-4171-839a-e323cc5fa369",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c47279e8-c6df-4cdb-bd50-c330a9a5dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-rnarad/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 01:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279051</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.597176</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.343848</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.578697</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.409616</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.603553</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.342777</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345435</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.587026</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.378324</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.329076</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.612337</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.326902</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.623007</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.315467</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.629449</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.324132</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.626456</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=0.09186248183250427, metrics={'train_runtime': 72.1308, 'train_samples_per_second': 34.659, 'train_steps_per_second': 4.436, 'total_flos': 164442933120000.0, 'train_loss': 0.09186248183250427, 'epoch': 10.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ec73b-3e91-4103-9d35-bb3740b22c5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af4a1f0f-ab1b-48d1-954f-ac84591095ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.27905139327049255,\n",
       " 'eval_f1': 0.6072423398328691,\n",
       " 'eval_roc_auc': 0.5971761337757824,\n",
       " 'eval_accuracy': 0.436,\n",
       " 'eval_runtime': 0.4974,\n",
       " 'eval_samples_per_second': 502.58,\n",
       " 'eval_steps_per_second': 64.33,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d881ab1-8326-4960-ba8e-166080652b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Nothing left to do but smile, smile, smile\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a91106c-2558-4800-8dcb-5e86a8f68439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.4482]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54d57e80-8ce1-4989-86c2-659f1c4a9049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90971750-9f38-40cf-8ac6-ea4cc2c3ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0') tensor(0.4482, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(labels[i], logits[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5bcde03-9cbd-4dd9-b31b-875f1d8c3875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3463852/2539642379.py:5: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
      "  predictions[np.where(probs >= 0.2)] = 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m probs \u001b[38;5;241m=\u001b[39m sigmoid(logits\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# turn predicted id's into actual label names\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [id2label[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions) \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.2)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2fc06-62a7-430f-9b0f-4615a3839dd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
